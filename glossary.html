<html>
  <head>
    <style>
      .linenums {
        list-style-type: none;
      }

      .formatted-line-numbers {
        display: none;
      }
      .action-code-block {
        display: none;
      }
      table {
        border-collapse: collapse;
        width: 100%;
      }
      table,
      th,
      td {
        border: 1px solid black;
        padding: 8px;
        text-align: left;
      }
    </style>
  </head>
  <body>
    <h2><span class="header-link octicon octicon-link"></span>Generative AI</h2>
    <h3>
      <span class="header-link octicon octicon-link"></span>Course Glossary
    </h3>
    <div>
      <p>
        Welcome! This alphabetized glossary contains many of the terms in this
        course. This comprehensive glossary also includes additional
        industry-recognized terms not used in course videos. These terms are
        essential for you to recognize when working in the industry,
        participating in user groups, and in other certificate programs.
      </p>
      <p><strong>Estimated reading time:</strong> 4 minutes</p>
      <table>
        <thead>
          <tr>
            <th>Term</th>
            <th>Definition</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Bidirectional and Auto-Regressive Transformers (BART)</td>
            <td>
              Sequence-to-sequence large language model (LLM), which follows an
              encoder-decoder architecture. It leverages encoding for contextual
              understanding and decoding to generate text.
            </td>
          </tr>
          <tr>
            <td>
              Bidirectional Encoder Representations from Transformers (BERT)
            </td>
            <td>
              Large language model (LLM), which utilizes an encoder-only
              transformer architecture. It is exceptional at understanding the
              context of a word within a sentence, which is crucial for nuanced
              tasks like sentiment analysis.
            </td>
          </tr>
          <tr>
            <td>
              Data analysis learning with language model for generation and
              exploration (DALL-E)
            </td>
            <td>
              AI model developed by OpenAI, known for generating complex and
              creative images from textual descriptions using deep learning
              techniques
            </td>
          </tr>
          <tr>
            <td>Data loader</td>
            <td>
              Application component that enables efficient batching and
              shuffling of data, which is essential for training neural
              networks. It allows for on-the-fly preprocessing, which optimizes
              memory usage.
            </td>
          </tr>
          <tr>
            <td>Data set</td>
            <td>Collection of data samples and their labels</td>
          </tr>
          <tr>
            <td>Diffusion model</td>
            <td>
              Probabilistic generative AI model commonly used for image
              generation. A diffusion model is trained to generate images by
              learning to remove noise or reconstruct examples from its training
              data that have been distorted beyond recognition.
            </td>
          </tr>
          <tr>
            <td>Fine-tuning</td>
            <td>
              Adjusting a pretrained model to improve performance for a specific
              task or data set. This makes the model generate more accurate and
              contextually relevant content.
            </td>
          </tr>
          <tr>
            <td>Generative adversarial network (GAN)</td>
            <td>
              Generative AI model that can generate images from random input
              vectors or seed images. It consists of a generator and a
              discriminator, which work in a competitive mode.
            </td>
          </tr>
          <tr>
            <td>Generative AI</td>
            <td>
              Deep-learning models that can generate high-quality text, images,
              and other content based on the data they were trained on. These
              models are developed and trained to understand patterns and
              structures within existing data and apply the understanding to
              produce new and relevant data.
            </td>
          </tr>
          <tr>
            <td>Generative pre-trained transformers (GPT)</td>
            <td>
              Generative AI model based on transformer architecture. It has been
              pretrained on large amounts of text data and can predict and
              generate text sequences based on the patterns learned from its
              training data.
            </td>
          </tr>
          <tr>
            <td>Hugging Face</td>
            <td>
              Platform that offers an open-source library with pretrained models
              and tools to streamline the process of training and fine-tuning
              generative AI models.
            </td>
          </tr>
          <tr>
            <td>Iterator</td>
            <td>
              An object that can be looped over. It contains elements that can
              be iterated through and typically includes two methods: iter and
              next.
            </td>
          </tr>
          <tr>
            <td>LangChain</td>
            <td>
              Open-source framework that helps in streamlining AI application
              development using large language models (LLMs)
            </td>
          </tr>
          <tr>
            <td>Large language models (LLMs)</td>
            <td>
              Foundation models that use AI and deep learning with vast data
              sets to generate text, translate languages, and create various
              types of content. They are called large language models due to the
              size of the training data set and the number of parameters.
            </td>
          </tr>
          <tr>
            <td>Natural language processing (NLP)</td>
            <td>
              Subfield of artificial intelligence (AI) that deals with the
              interaction of computers and humans in human language. It involves
              creating algorithms and models that will help computers understand
              and comprehend human language and generate contextually relevant
              text in human language.
            </td>
          </tr>
          <tr>
            <td>NLTK</td>
            <td>
              Python library used in natural language processing (NLP) for tasks
              such as tokenization and text processing
            </td>
          </tr>
          <tr>
            <td>Pydantic</td>
            <td>
              Python library that helps streamline data handling. It can be used
              to parse and validate your data.
            </td>
          </tr>
          <tr>
            <td>PyTorch</td>
            <td>
              Dynamic deep learning framework developed by Facebook's AI
              Research lab. It is a Python-based library well-known for its ease
              of use, flexibility, and dynamic computation graphs.
            </td>
          </tr>
          <tr>
            <td>Recurrent neural networks (or RNNs)</td>
            <td>
              Artificial neural networks that use sequential or time series
              data. RNNs are used to solve data-related problems with a natural
              order or time-based dependencies.
            </td>
          </tr>
          <tr>
            <td>SentencePiece</td>
            <td>
              Subword-based tokenization algorithm that segments text into
              manageable parts and assigns unique IDs
            </td>
          </tr>
          <tr>
            <td>spaCy</td>
            <td>
              Open-source library used in natural language processing. It
              provides tools for tasks such as tokenization and word embeddings.
            </td>
          </tr>
          <tr>
            <td>TensorFlow</td>
            <td>
              Open-source machine learning framework. It provides a set of tools
              and libraries to facilitate the development and deployment of
              machine learning models.
            </td>
          </tr>
          <tr>
            <td>Text-to-Text Transfer Transformer (T5)</td>
            <td>
              Transformer-based large language model, which uses a text-to-text
              framework. It leverages encoding for contextual understanding and
              decoding to generate text.
            </td>
          </tr>
          <tr>
            <td>Tokenization</td>
            <td>
              Breaking text into smaller pieces or tokens. The tokens help a
              generative AI model understand the text better.
            </td>
          </tr>
          <tr>
            <td>Tokenizer</td>
            <td>Program that breaks down text into individual tokens</td>
          </tr>
          <tr>
            <td>Transformers</td>
            <td>
              Deep learning models that can translate text and speech in
              near-real-time. They take data, such as words or numbers, and pass
              it through different layers, with information flowing in one
              direction.
            </td>
          </tr>
          <tr>
            <td>Unigram</td>
            <td>
              Subword-based tokenization algorithm that breaks text into smaller
              pieces. It begins with a large list of possibilities and gradually
              narrows down based on how frequently they appear in the text.
            </td>
          </tr>
          <tr>
            <td>variational autoencoders (VAEs)</td>
            <td>
              Generative AI model that operates on an encoder-decoder framework.
              The encoder network first compresses input data into a simplified,
              abstract space that captures essential characteristics. The
              decoder network then uses this condensed information to recreate
              the original data.
            </td>
          </tr>
          <tr>
            <td>WaveNet</td>
            <td>
              Generative AI model designed for generating audio content. It can
              be used for tasks such as speech synthesis.
            </td>
          </tr>
          <tr>
            <td>WordPiece</td>
            <td>
              Subword-based tokenization algorithm that evaluates the benefits
              and drawbacks of splitting and merging two symbols to ensure its
              decisions are valuable.
            </td>
          </tr>
        </tbody>
      </table>

      <p>
        <img
          src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nRmYgyM2KjRIIiG16R7ikg/ibmsn-footer-blue.png"
          alt=""
        />
      </p>
    </div>
  </body>
</html>
