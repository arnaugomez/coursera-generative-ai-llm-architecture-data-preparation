<html>
  <head>
    <style>
      .linenums {
        list-style-type: none;
      }

      .formatted-line-numbers {
        display: none;
      }
      .action-code-block {
        display: none;
      }
      table {
        border-collapse: collapse;
        width: 100%;
      }
      table,
      th,
      td {
        border: 1px solid black;
        padding: 8px;
        text-align: left;
      }
    </style>
  </head>
  <body>
    <h2>
      <span class="header-link octicon octicon-link"></span>Basics of AI
      Hallucinations
    </h2>
    <h4><span class="header-link octicon octicon-link"></span>Objectives</h4>
    <p>After completing this reading, you will be able to:</p>
    <ul>
      <li>Define AI hallucinations.</li>
      <li>
        List the problems caused by AI hallucinations and the ways of preventing
        these.
      </li>
    </ul>
    <p><strong>Estimated reading time:</strong> 10 minutes</p>
    <h3><span class="header-link octicon octicon-link"></span>Introduction</h3>
    <p>
      You can utilize large language models (LLMs) to generate authoritative
      text across domains. However, they may generate information that sounds
      right but is inaccurate. They may also produce biased content. These
      problems can be a result of AI hallucinations. In this reading, you will
      learn about AI hallucinations.
    </p>
    <h3>
      <span class="header-link octicon octicon-link"></span>AI hallucinations
    </h3>
    <p>
      In AI hallucinations, the model generates output that it presents as
      accurate but is seen as unrealistic, inaccurate, irrelevant, or
      nonsensical by humans. It is similar to the way humans experience
      hallucinations.
    </p>
    <p>
      For example, there was an incident where ChatGPT falsely claimed that a
      mayor in Australia was found guilty and imprisoned in a bribery case. In
      reality, the mayor notified the authorities about a bribery issue.
      (Reference:
      <a
        href="https://www.reuters.com/technology/australian-mayor-readies-worlds-first-defamation-lawsuit-over-chatgpt-content-2023-04-05/"
        target="_blank"
        rel="noopener noreferrer"
        title="Australian mayor readies worldâ€™s first defamation lawsuit over ChatGPT content | Reuters"
        >Australian mayor readies world's first defamation lawsuit over ChatGPT
        content | Reuters</a
      >)
    </p>
    <p>
      The example shows that there can be a significant implication of AI
      hallucinations. However, note that such incidents are rare and isolated.
    </p>
    <p>
      AI hallucinations are strongly associated with LLMs. Factors such as
      biases in the training data, limited training, complexity of the model,
      and lack of human oversight can cause AI hallucinations. Also, the outputs
      generated by the AI models might not be based on the patterns the models
      learned from the training data.
    </p>
    <h3>
      <span class="header-link octicon octicon-link"></span>Problems caused by
      AI hallucinations
    </h3>
    <p>
      AI hallucinations can have serious implications. For example, if an LLM
      summarizes pages of a legal document incorrectly, it can lead to legal
      disputes and litigation.
    </p>
    <p>Some of the problems caused are:</p>
    <ul>
      <li>Generation of inaccurate information</li>
      <li>Creation of biased views or misleading information</li>
      <li>
        Wrong input provided to sensitive applications, such as those used in
        autonomous vehicles or medical domain
      </li>
    </ul>
    <h3>
      <span class="header-link octicon octicon-link"></span>Methods for
      mitigating hallucinations
    </h3>
    <ul>
      <li>
        Eliminating any bias in the training data and performing extensive
        training of the models on high-quality data
      </li>
      <li>Avoiding manipulation of the inputs that are fed into the models</li>
      <li>Ongoing evaluation and improvement of the models</li>
      <li>Fine-tuning a pre-trained LLM on domain-specific data</li>
    </ul>
    <h3>
      <span class="header-link octicon octicon-link"></span>Preventing the
      problems caused by AI hallucinations
    </h3>
    <p>
      It is inevitable for hallucinations to occur within LLMs. What can be
      frustrating is that the generated text often contains subtle mistakes that
      are challenging to identify. There are a couple of best practices that you
      can follow. These include:
    </p>
    <ul>
      <li>
        Being vigilant and understanding that these models do not understand the
        actual meaning of the words but are focused on predicting the next word
        in a sequence based on patterns. These models are trained on vast
        amounts of data and learn statistical patterns, but they lack semantic
        understanding or comprehension <strong>like human beings</strong>.
      </li>
      <li>
        Ensuring human oversight regularly for fact-checking and continuous
        testing
      </li>
      <li>
        Providing additional context in the prompt or input. This will enable
        LLMs to understand the desired output better and generate more accurate
        and contextually relevant responses.
      </li>
    </ul>
    <h2><span class="header-link octicon octicon-link"></span>Summary</h2>
    <p>In this reading, you learned that:</p>
    <ul>
      <li>
        AI hallucinations refer to an AI model generating output presented as
        accurate but seen as unrealistic, inaccurate, irrelevant, or nonsensical
        by humans.
      </li>
      <li>
        AI hallucination can result in the generation of inaccurate information,
        the creation of biased views, and wrong input provided to sensitive
        applications.
      </li>
      <li>
        You can prevent the problems caused by AI hallucinations through:
        <ul>
          <li>Extensive training with high-quality data,</li>
          <li>Avoiding manipulation,</li>
          <li>Ongoing evaluation and improvement of the models,</li>
          <li>Fine-tuning on domain-specific data,</li>
          <li>Being vigilant,</li>
          <li>Ensuring human oversight, and</li>
          <li>Providing additional context in the prompt.</li>
        </ul>
      </li>
    </ul>
    <footer>
      <img
        src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/1-MVUWz-1NEQypgkcpfHzA/ibmsn-footer.png"
        alt=""
      />
    </footer>
  </body>
</html>
